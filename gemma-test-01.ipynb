{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-01T06:33:06.432498Z","iopub.execute_input":"2024-03-01T06:33:06.432769Z","iopub.status.idle":"2024-03-01T06:33:07.536734Z","shell.execute_reply.started":"2024-03-01T06:33:06.432744Z","shell.execute_reply":"2024-03-01T06:33:07.535818Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/gemma/pytorch/2b/2/config.json\n/kaggle/input/gemma/pytorch/2b/2/gemma-2b.ckpt\n/kaggle/input/gemma/pytorch/2b/2/tokenizer.model\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setup the environment\n!pip install -q -U immutabledict sentencepiece \n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:33:46.325592Z","iopub.execute_input":"2024-03-01T06:33:46.326185Z","iopub.status.idle":"2024-03-01T06:34:04.130396Z","shell.execute_reply.started":"2024-03-01T06:33:46.326152Z","shell.execute_reply":"2024-03-01T06:34:04.129272Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'gemma_pytorch'...\nremote: Enumerating objects: 71, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 71 (delta 12), reused 8 (delta 8), pack-reused 55\u001b[K\nUnpacking objects: 100% (71/71), 2.13 MiB | 5.19 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working/gemma_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:35:29.260059Z","iopub.execute_input":"2024-03-01T06:35:29.260438Z","iopub.status.idle":"2024-03-01T06:35:30.249227Z","shell.execute_reply.started":"2024-03-01T06:35:29.260406Z","shell.execute_reply":"2024-03-01T06:35:30.248324Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CONTRIBUTING.md  README.md  gemma\t      scripts\ttokenizer\nLICENSE\t\t docker     requirements.txt  setup.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys \nsys.path.append(\"/kaggle/working/gemma_pytorch/\") \nfrom gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:35:41.120197Z","iopub.execute_input":"2024-03-01T06:35:41.121029Z","iopub.status.idle":"2024-03-01T06:35:44.450475Z","shell.execute_reply.started":"2024-03-01T06:35:41.120979Z","shell.execute_reply":"2024-03-01T06:35:44.449528Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load the model\nVARIANT = \"2b\" \nMACHINE_TYPE = \"cpu\" \nweights_dir = '/kaggle/input/gemma/pytorch/2b/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n  model.load_weights(ckpt_path)\n  model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:44:30.237277Z","iopub.execute_input":"2024-02-29T07:44:30.237795Z","iopub.status.idle":"2024-02-29T07:45:01.076007Z","shell.execute_reply.started":"2024-02-29T07:44:30.237765Z","shell.execute_reply":"2024-02-29T07:45:01.075100Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A PyTorch implementation of Gemma 2B model. It is a 2B (two billion) parameter base model that has not yet been instruction-tuned.\n\nPre-trained (PT) models can be used as base models for further development, while instruction-tuned (IT) variants can be used for chatting and following prompts.\n\nInformation on instruction-tuned: https://www.linkedin.com/pulse/generative-ai-executives-10-minute-deep-dive-amit-gupta/\n\nThe IT variant is available on the same kaggle model page:  https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/2b-it\n\nIt summarizes the differences:  \"Pre-trained (PT) models can be used as base models for further development, while instruction-tuned (IT) variants can be used for chatting and following prompts.\"","metadata":{}},{"cell_type":"code","source":"# Load the model\nVARIANT = \"2b-it\" \n# Need to set this to cuda, not gpu or cpu while using the gpu t4 on kaggle.\n# Much faster results (as expected) when I did so.\nMACHINE_TYPE = \"cuda\" \nweights_dir = '/kaggle/input/gemma/pytorch/2b-it/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n  model.load_weights(ckpt_path)\n  model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:41:14.356124Z","iopub.execute_input":"2024-03-01T06:41:14.357111Z","iopub.status.idle":"2024-03-01T06:41:14.434951Z","shell.execute_reply.started":"2024-03-01T06:41:14.357063Z","shell.execute_reply":"2024-03-01T06:41:14.433677Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(MACHINE_TYPE)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_default_tensor_type(model_config\u001b[38;5;241m.\u001b[39mget_dtype()):\n\u001b[0;32m---> 20\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mGemmaForCausalLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m   ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVARIANT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_weights(ckpt_path)\n","File \u001b[0;32m/kaggle/working/gemma/model.py:400\u001b[0m, in \u001b[0;36mGemmaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m head_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m    398\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m Embedding(vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mquant)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m GemmaModel(config)\n","File \u001b[0;32m/kaggle/working/gemma/tokenizer.py:24\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: Optional[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Reload tokenizer.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(model_path), model_path\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m SentencePieceProcessor(model_file\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# BOS / EOS token IDs.\u001b[39;00m\n","\u001b[0;31mAssertionError\u001b[0m: /kaggle/input/gemma/pytorch/2b-it/2/tokenizer.model"],"ename":"AssertionError","evalue":"/kaggle/input/gemma/pytorch/2b-it/2/tokenizer.model","output_type":"error"}]},{"cell_type":"code","source":"!ls /kaggle/input/gemma/pytorch/2b/2","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:41:49.653722Z","iopub.execute_input":"2024-03-01T06:41:49.654664Z","iopub.status.idle":"2024-03-01T06:41:50.657107Z","shell.execute_reply.started":"2024-03-01T06:41:49.654625Z","shell.execute_reply":"2024-03-01T06:41:50.655848Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"config.json  gemma-2b.ckpt  tokenizer.model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Had to follow the advice on this link to get partially reasonable results:\n\nhttps://www.kaggle.com/models/google/gemma/discussion/478675\n\nStill hard to interpret.  I will have to dig into Gemma more...","metadata":{}},{"cell_type":"code","source":"# Use the model\n\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\nMODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn>\\n\"\n\nprompt = (\n    USER_CHAT_TEMPLATE.format(\n        prompt=\"What is a good place for travel in the US?\"\n    )\n    + MODEL_CHAT_TEMPLATE.format(prompt=\"Ohio.\")\n    + USER_CHAT_TEMPLATE.format(prompt=\"What can I do in Ohio?\")\n    + \"<start_of_turn>model\\n\"\n)\n\nmodel.generate(\n    prompt,\n    device=device,\n    output_len=100,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:11:06.129852Z","iopub.execute_input":"2024-02-29T08:11:06.130226Z","iopub.status.idle":"2024-02-29T08:11:09.062613Z","shell.execute_reply.started":"2024-02-29T08:11:06.130196Z","shell.execute_reply":"2024-02-29T08:11:09.061620Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'Just drive.Throwaway cities—Cincinnati, Columbus and Cleveland—blend in well with that stretch of the Appalachian mountains and the Great Lakes.\\n==\"\"){==\"\"){==\"\"){==\"\"){==\"\")==\"\"){==\"\"){==\"\"){==\"\")==\"\"){==\"\"){==\"\"){==\"\"){==\"\"){==\"\"){==\"\"){==\"\"){\\n==\"\"){\\n{==\"\"){==\"\"){==\"\"){{==\"\"){\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"Results are bad to OK.  Not sure that they are entirely correct. Will need to dig into the model documentation more to understand how this might be tuned, and what the separator (or filler) words are. Sometimes the output is primarily these seemingly random filler words. Maybe the system can't think of anything else to say?","metadata":{}}]}