{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-01T06:33:06.432498Z","iopub.execute_input":"2024-03-01T06:33:06.432769Z","iopub.status.idle":"2024-03-01T06:33:07.536734Z","shell.execute_reply.started":"2024-03-01T06:33:06.432744Z","shell.execute_reply":"2024-03-01T06:33:07.535818Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/gemma/pytorch/2b/2/config.json\n/kaggle/input/gemma/pytorch/2b/2/gemma-2b.ckpt\n/kaggle/input/gemma/pytorch/2b/2/tokenizer.model\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setup the environment\n!pip install -q -U immutabledict sentencepiece \n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:33:46.325592Z","iopub.execute_input":"2024-03-01T06:33:46.326185Z","iopub.status.idle":"2024-03-01T06:34:04.130396Z","shell.execute_reply.started":"2024-03-01T06:33:46.326152Z","shell.execute_reply":"2024-03-01T06:34:04.129272Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'gemma_pytorch'...\nremote: Enumerating objects: 71, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 71 (delta 12), reused 8 (delta 8), pack-reused 55\u001b[K\nUnpacking objects: 100% (71/71), 2.13 MiB | 5.19 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working/gemma_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:35:29.260059Z","iopub.execute_input":"2024-03-01T06:35:29.260438Z","iopub.status.idle":"2024-03-01T06:35:30.249227Z","shell.execute_reply.started":"2024-03-01T06:35:29.260406Z","shell.execute_reply":"2024-03-01T06:35:30.248324Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CONTRIBUTING.md  README.md  gemma\t      scripts\ttokenizer\nLICENSE\t\t docker     requirements.txt  setup.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys \nsys.path.append(\"/kaggle/working/gemma_pytorch/\") \nfrom gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:35:41.120197Z","iopub.execute_input":"2024-03-01T06:35:41.121029Z","iopub.status.idle":"2024-03-01T06:35:44.450475Z","shell.execute_reply.started":"2024-03-01T06:35:41.120979Z","shell.execute_reply":"2024-03-01T06:35:44.449528Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load the model\nVARIANT = \"2b\" \nMACHINE_TYPE = \"cpu\" \nweights_dir = '/kaggle/input/gemma/pytorch/2b/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n  model.load_weights(ckpt_path)\n  model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:44:30.237277Z","iopub.execute_input":"2024-02-29T07:44:30.237795Z","iopub.status.idle":"2024-02-29T07:45:01.076007Z","shell.execute_reply.started":"2024-02-29T07:44:30.237765Z","shell.execute_reply":"2024-02-29T07:45:01.075100Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A PyTorch implementation of Gemma 2B model. It is a 2B (two billion) parameter base model that has not yet been instruction-tuned.\n\nPre-trained (PT) models can be used as base models for further development, while instruction-tuned (IT) variants can be used for chatting and following prompts.\n\nInformation on instruction-tuned: https://www.linkedin.com/pulse/generative-ai-executives-10-minute-deep-dive-amit-gupta/\n\nThe IT variant is available on the same kaggle model page:  https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/2b-it\n\nIt summarizes the differences:  \"Pre-trained (PT) models can be used as base models for further development, while instruction-tuned (IT) variants can be used for chatting and following prompts.\"","metadata":{}},{"cell_type":"markdown","source":"Note that I had to add the model for the IT variant to this notebook (only the 2b version was originally added) through the Notebook pane, on the right, with the 'Add Input' button.","metadata":{}},{"cell_type":"code","source":"# Load the model\nVARIANT = \"2b-it\" \n# Need to set this to cuda, not gpu or cpu while using the gpu t4 on kaggle.\n# Much faster results (as expected) when I did so.\nMACHINE_TYPE = \"cuda\" \nweights_dir = '/kaggle/input/gemma/pytorch/2b-it/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n  model.load_weights(ckpt_path)\n  model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:47:32.822146Z","iopub.execute_input":"2024-03-01T06:47:32.823049Z","iopub.status.idle":"2024-03-01T06:48:22.377946Z","shell.execute_reply.started":"2024-03-01T06:47:32.823011Z","shell.execute_reply":"2024-03-01T06:48:22.376761Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/gemma/pytorch/2b-it/2","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:48:47.213867Z","iopub.execute_input":"2024-03-01T06:48:47.214218Z","iopub.status.idle":"2024-03-01T06:48:48.197321Z","shell.execute_reply.started":"2024-03-01T06:48:47.214192Z","shell.execute_reply":"2024-03-01T06:48:48.196065Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"config.json  gemma-2b-it.ckpt  tokenizer.model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Had to follow the advice on this link to get partially reasonable results:\n\nhttps://www.kaggle.com/models/google/gemma/discussion/478675\n\nStill hard to interpret.  I will have to dig into Gemma more...","metadata":{}},{"cell_type":"code","source":"# Use the model\n\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\nMODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn>\\n\"\n\nprompt = (\n    USER_CHAT_TEMPLATE.format(\n        prompt=\"Give me a workout for beginners\"\n    )\n    + \"<start_of_turn>model\\n\"\n)\n\nmodel.generate(\n    prompt,\n    device=device,\n    output_len=100,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:49:53.592407Z","iopub.execute_input":"2024-03-01T06:49:53.593459Z","iopub.status.idle":"2024-03-01T06:49:56.700364Z","shell.execute_reply.started":"2024-03-01T06:49:53.593420Z","shell.execute_reply":"2024-03-01T06:49:56.699358Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'**Beginner Workout**\\n\\n**Warm-up (5 minutes)**\\n\\n* Light cardio, such as brisk walking or cycling\\n* Dynamic stretching, such as arm circles, torso twists, and leg swings\\n\\n**Core workout (15 minutes)**\\n\\n* Crunches: 3 sets of 10-12 repetitions\\n* Plank: 3 sets of 30-60 seconds\\n* Squats: 3 sets of 10-12 repetitions\\n* Side'"},"metadata":{}}]},{"cell_type":"markdown","source":"Results are bad to OK.  Not sure that they are entirely correct. Will need to dig into the model documentation more to understand how this might be tuned, and what the separator (or filler) words are. Sometimes the output is primarily these seemingly random filler words. Maybe the system can't think of anything else to say?","metadata":{}},{"cell_type":"markdown","source":"Update: By switching to the 2b-it variant I now seem to get much better results. This is not surprising as the IT variant is designed for this sort of query pattern. The odd thing is that the 2b version presents the same queries, but it works poorly with them. Worth investigating that some more!","metadata":{}}]}